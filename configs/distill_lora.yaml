# Configuration for LoRA student distillation training

# Dataset configuration
dataset_name: "lambdalabs/pokemon-blip-captions"  # Hugging Face dataset name
# data_dir: null  # Alternative: local directory path with images/captions

# Training configuration
output_dir: "./checkpoints"
run_name: "lora_student"
resolution: 512
batch_size: 1
gradient_accumulation_steps: 4
max_steps: 2000
learning_rate: 1.0e-4

# Model configuration
base_model_id: "runwayml/stable-diffusion-v1-5"
lora_rank: 4
lora_alpha: 32
lora_dropout: 0.0

# Distillation configuration
teacher_steps: 50  # Number of steps for teacher inference
student_steps: 4   # Number of steps for student inference
guidance_scale: 7.5  # Guidance scale for teacher

# Other
seed: 42
